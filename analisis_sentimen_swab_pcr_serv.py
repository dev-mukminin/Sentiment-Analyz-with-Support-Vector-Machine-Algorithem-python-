# -*- coding: utf-8 -*-
"""Analisis Sentimen swab-pcr serv

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1njipD9MNvoW-o-Ts07aEIKo1PuuK_Duz
"""

import pandas as pd
df_crawling = pd.read_csv("/content/dataset.csv")

df_crawling.tail()

Label = []
for index, row in df_crawling.iterrows():
    if row["Label"] == "POSITIF":
        Label.append(1)
    else:
        Label.append(0)

df_crawling["Label"] = Label
df_crawling

df_preprocessed = df_crawling.copy()
df_preprocessed.head()

import string, re

def cleansing(data):
    # remove link
    data = re.sub(r'https?:\/\/\S+',r'', data)

    # removed @mentions  
    data = re.sub(r'@[A-Za-z0-9]+',r'', data)

    #removing RT
    data = re.sub(r'RT[\s]+',r'', data)

    # remove number
    data = re.sub(r'[0-9]+',r'', data)
    
    # remove punctuation
    remove = string.punctuation
    translator = str.maketrans(remove, ' '*len(remove))
    data = data.translate(translator)
    
    # remove newline
    data = data.replace('\n', ' ')

    # remove space
    data = data.replace('[\s]+','')
    
    return data

tweet = []
for index, row in df_preprocessed.iterrows():
    tweet.append(cleansing(row["Text"]))
    
df_preprocessed["Text"] = tweet
df_preprocessed.head()

#casefolding

df_preprocessed["Text"] = df_preprocessed["Text"].astype(str).str.lower()
df_preprocessed.head()

# this function

import nltk

#tokenizing import
#a library
# melakukan import fungsi tokenizer

from nltk.tokenize import TweetTokenizer

#  memasukan fungsi tweet.tokenizer kedalam variabel data
 
def tokenizing(data):
  tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
  data = tokenizer.tokenize(data)

  return data

# jalankan tokenizing
# proses tokenizing pada kolom "text"

tweet = []
for index, row in df_preprocessed.iterrows():
    tweet.append(tokenizing(row["Text"]))
    
df_preprocessed["Text"] = tweet
df_preprocessed.head()

def hapus_tandabaca(data):
  data=' '.join([char for char in data if char not in string.punctuation])
  return data

tweet = []
for index, row in df_preprocessed.iterrows():
    tweet.append(hapus_tandabaca(row["Text"]))

df_preprocessed["Text"]= tweet
df_preprocessed.head()

pip install PySastrawi

nltk.download('stopwords')

# import library stopword
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
 
factory = StopWordRemoverFactory()
stopword = factory.create_stop_word_remover()

tweet = []
for index, row in df_preprocessed.iterrows():
    tweet.append(stopword.remove(row["Text"]))
    
df_preprocessed["Text"] = tweet
df_preprocessed.head()

# stemming
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
factory = StemmerFactory()
stemmer = factory.create_stemmer()

tweet = []
for index, row in df_preprocessed.iterrows():
    tweet.append(stemmer.stem(row["Text"]))
    
df_preprocessed["Text"] = tweet
df_preprocessed.head()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df_preprocessed['Text'], df_preprocessed['Label'], 
                                                    test_size=0.2, stratify=df_preprocessed['Label'], random_state=20)

X_train, X_test, y_train, y_test

data_testing = X_test.copy()
f_test = df_preprocessed['Text']
f_test

from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()

x_train = vectorizer.fit_transform(X_train)
x_test = vectorizer.transform(X_test)
f_test = vectorizer.transform(f_test)

print(x_train.shape)
print(x_test.shape)
print(f_test.shape)

term = vectorizer.get_feature_names()

vectorizer.get_feature_names()

X_train = X_train.to_excel("/content/D_training.xlsx")

import csv

with open('/content/term.csv', "w") as f:
    writer = csv.writer(f)
    for row in term:
        writer.writerow(row)

print(x_train)

print(y_train)

from sklearn import svm
from sklearn.model_selection import cross_val_score

clf = svm.SVC(kernel="linear")

scores = cross_val_score(clf, x_train, y_train, cv=10)

from sklearn.model_selection import cross_val_score

scores = cross_val_score(clf, x_train, y_train, cv=10)

scores= list(map('{:.1%}'.format,scores))
print(f'{scores}')

print(f"{cross_val_score(clf, x_train, y_train, cv=10).mean():.1%}")

# lakukan prediksi pada data test

import sys
import numpy
numpy.set_printoptions(threshold=sys.maxsize)
clf.fit(x_train,y_train)
predict = clf.predict(x_test)

print(predict)
len(predict)

# import library evaluation

from sklearn.metrics import f1_score, recall_score, precision_score, confusion_matrix, accuracy_score

# f1_score
print("f1 score hasil prediksi \t: ", (f"{f1_score(y_test, predict):.2%}"))

# accuracy score
print("accuracy score hasil prediksi \t: ", (f"{accuracy_score(y_test, predict):.2%}"))

# precision score
print("precision score hasil prediksi \t: ", (f"{precision_score(y_test, predict):.2%}"))

# recall score
print("recall score hasil prediksi \t: ", (f"{recall_score(y_test, predict):.2%}"))

# confusion matrix

tn, fp, fn, tp = confusion_matrix(y_test, predict).ravel()

# True Negative
print("Hasil True Negative \t: ", (tn))

# Flse Positive
print("Hasil False Positive \t: ", (fp))

# False Negative
print("Hasil False Negative \t: ", (fn))

# True Positive
print("Hasil True Positive \t: ",(tp))

f_pred_test = clf.predict(f_test)
print(f_pred_test)
len(f_pred_test)

df_datatesting = pd.DataFrame(data_testing)
df_Labeltesting = pd.DataFrame(predict)
df_datafull = df_preprocessed['Text']
df_Labelfull = pd.DataFrame(f_pred_test)

df_datatesting

df_Labeltesting.head()

Label = []
for index, row in df_Labelfull.iterrows():
    if row[0] == 1:
        Label.append("POSITIF")
    else:
        Label.append("NEGATIF")

df_Labelfull["Label"] = Label
df_Labelfull = df_Labelfull.drop(columns=[0])
df_Labelfull.head()

Label = []
for index, row in df_Labeltesting.iterrows():
    if row[0] == 1:
        Label.append("POSITIF")
    else:
        Label.append("NEGATIF")

df_Labeltesting["Label"] = Label
df_Labeltesting = df_Labeltesting.drop(columns=[0])
df_Labeltesting.head()

pip install XlsxWriter

# Creating Excel Writer Object from Pandas  

writer = pd.ExcelWriter('dataout.xlsx', engine='xlsxwriter')   
workbook=writer.book
worksheet=workbook.add_worksheet('Validation')
writer.sheets['Validation'] = worksheet
df_datafull.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)
df_Labelfull.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=2)
writer.save()

# Creating Excel Writer Object from Pandas  

writer = pd.ExcelWriter('testingout.xlsx', engine='xlsxwriter')   
workbook=writer.book
worksheet=workbook.add_worksheet('Validation')
writer.sheets['Validation'] = worksheet
df_datatesting.to_excel(writer,sheet_name='Validation',startrow=0 , startcol=0)   
df_Labeltesting.to_excel(writer,sheet_name='Validation',startrow=0, startcol=2) 
writer.save()

dft = pd.read_excel("/content/testingout.xlsx")
dft = dft.drop(columns=['Unnamed: 0', 'Unnamed: 2'])
dft

dft['Label'].value_counts()

dft = pd.read_excel("/content/dataout.xlsx")
dft = dft.drop(columns=['Unnamed: 0', 'Unnamed: 2'])
dft

dft_diagram = dft.copy()
dft

Label = []
for index, row in dft.iterrows():
    if row["Label"] == "POSITIF":
        Label.append(1)
    else:
        Label.append(0)

dft["Label"] = Label
dft

dft["Text"] = dft["Text"].astype(str)

from nltk.corpus import stopwords
import nltk
nltk.download("stopwords")

pos = pd.read_csv("/content/positive.txt", delimiter = "\t")
pos

neg = pd.read_csv("/content/negative.txt", delimiter = "\t")
neg

from wordcloud import WordCloud
import matplotlib.pyplot as plt

# Polarity == 0 negative

train = dft[dft["Label"] == 0]
all_text = ' '.join(word for word in train["Text"])
stop_words = ["layan","swab","pcr"]
wordcloud = WordCloud(stopwords = stop_words, colormap='Reds', width=1000, height=1000, 
                      mode='RGBA', background_color='white').generate(all_text)
plt.figure(figsize=(5,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()

# Polarity == 1 positive

train_s1 = dft[dft["Label"] == 1]
all_text_s1 = ' '.join(word for word in train_s1["Text"])
stop_words = ["layan","swab","pcr"]
wordcloud = WordCloud(stopwords= stop_words, width=1000, height=1000, colormap='Blues', 
                      background_color='white', mode='RGBA').generate(all_text_s1)
plt.figure(figsize=(5,5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.margins(x=0, y=0)
plt.show()

from collections import Counter
print(Counter(dft_diagram['Label']))

title_type = dft_diagram.groupby('Label').agg('count')
print(title_type)

import matplotlib.cm as cm
import matplotlib as mpl
from matplotlib.gridspec import GridSpec
import matplotlib.pyplot as plt

type_labels = title_type.Text.sort_values().index 
type_counts = title_type.Text.sort_values()

type_counts

# plt.figure(1, figsize=(5,5))

cmap = plt.get_cmap('Spectral')
cl = ['purple','orange']
plt.subplot(the_grid[1, 1], aspect=0)
type_show_ids = plt.pie(type_counts, labels=type_labels, autopct='%1.1f%%', 
                        shadow=True, colors=cl, radius=5)
plt.show()